{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clasificación de Text\n",
    "Créditos: https://keras.rstudio.com/articles/tutorial_basic_text_classification.html\n",
    "\n",
    "\n",
    "El siguiente tutorial consisten en clasificar reseñas de peliculas como  positivas o negativas utilizando el texto de la reseña. Este es un ejemplo de clasificacion binaria, un tipo de problema de aprendizaje automatico importante y apliamente aplicable.\n",
    "\n",
    "Se utilizara el conjunto de datos IMDB que contiene el texto de 50,000 reseñas de peliculas de la base de datos de internet. Estas se dividen en 25,000 reseñas para el entrenamiento y 25,000 reseñas para la validacion. Los conjuntos de entrenamiento y validacion son equilibrados, lo que significa que contiene el mismo numero de reseñas positivas y negativas.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Librerias necesarias\n",
    "\n",
    "library(keras)#necesario usar la funcion install_kereas() para instalar kerear, si no funcinoa intalar r studio en conda\n",
    "library(dplyr)\n",
    "library(ggplot2)\n",
    "library(purrr)\n",
    "library(tensorflow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Descargar conjuntos de datos IMDB\n",
    "\n",
    "El conjunto de datos IMDP vine con la libreria keras. Ya se ha procesados de manera tal que las reseñas (secuencias de palabras) se han convertido en secuencias de enteros, donde cada entero representa un palabra especifica en un diccionario.\n",
    "\n",
    "El siguiente codigo descarga eñ conjunto de datos IMDB (o usa una copia en cache si ya se ha descargado)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "imdb<-dataset_imdb(num_words = 10000)\n",
    "\n",
    "c(train_data, train_labels) %<-% imdb$train\n",
    "c(test_data, test_labels) %<-% imdb$test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El argumento num_words = 10000 mantiene las 10,000 palabras mas frecuentes en los datos de entrenamiento. Las plabras raras se descartan para mantener manejable el tamaño de los datos.\n",
    "\n",
    "Convenientemente, el conjunto de datos viene con un indice que asigna palabras a enteros, que deben descargarse por separado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<dl>\n",
       "\t<dt>$fawn</dt>\n",
       "\t\t<dd>34701</dd>\n",
       "\t<dt>$tsukino</dt>\n",
       "\t\t<dd>52006</dd>\n",
       "\t<dt>$nunnery</dt>\n",
       "\t\t<dd>52007</dd>\n",
       "</dl>\n"
      ],
      "text/latex": [
       "\\begin{description}\n",
       "\\item[\\$fawn] 34701\n",
       "\\item[\\$tsukino] 52006\n",
       "\\item[\\$nunnery] 52007\n",
       "\\end{description}\n"
      ],
      "text/markdown": [
       "$fawn\n",
       ":   34701\n",
       "$tsukino\n",
       ":   52006\n",
       "$nunnery\n",
       ":   52007\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "$fawn\n",
       "[1] 34701\n",
       "\n",
       "$tsukino\n",
       "[1] 52006\n",
       "\n",
       "$nunnery\n",
       "[1] 52007\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "word_index<-dataset_imdb_word_index()\n",
    "head(word_index,3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploracion de datos \n",
    "\n",
    "Tomemos un poco de tiempo para entender el formato de los datos. El conjunto de datos viene preprocesado: cada ejemplo es un conjunto de enteros que representan las palabras de la reseña de la pelicula. Cada etiqueta es un valor entero de 0 o 1, donde 0 es una reseña negativa y 1 es una reseña positiva."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "'Training entries: 25000, labels: 25000'"
      ],
      "text/latex": [
       "'Training entries: 25000, labels: 25000'"
      ],
      "text/markdown": [
       "'Training entries: 25000, labels: 25000'"
      ],
      "text/plain": [
       "[1] \"Training entries: 25000, labels: 25000\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "paste0(\"Training entries: \", length(train_data), \", labels: \", length(train_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los textos de la reseñas se han tranformado en numeros enteros, donde cada numero entero representa una palabra especifica en un diccionario. Asi es como se ve la primera reseña\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<ol class=list-inline>\n",
       "\t<li>1</li>\n",
       "\t<li>14</li>\n",
       "\t<li>22</li>\n",
       "\t<li>16</li>\n",
       "\t<li>43</li>\n",
       "\t<li>530</li>\n",
       "\t<li>973</li>\n",
       "\t<li>1622</li>\n",
       "\t<li>1385</li>\n",
       "\t<li>65</li>\n",
       "\t<li>458</li>\n",
       "\t<li>4468</li>\n",
       "\t<li>66</li>\n",
       "\t<li>3941</li>\n",
       "\t<li>4</li>\n",
       "\t<li>173</li>\n",
       "\t<li>36</li>\n",
       "\t<li>256</li>\n",
       "\t<li>5</li>\n",
       "\t<li>25</li>\n",
       "\t<li>100</li>\n",
       "\t<li>43</li>\n",
       "\t<li>838</li>\n",
       "\t<li>112</li>\n",
       "\t<li>50</li>\n",
       "\t<li>670</li>\n",
       "\t<li>2</li>\n",
       "\t<li>9</li>\n",
       "\t<li>35</li>\n",
       "\t<li>480</li>\n",
       "\t<li>284</li>\n",
       "\t<li>5</li>\n",
       "\t<li>150</li>\n",
       "\t<li>4</li>\n",
       "\t<li>172</li>\n",
       "\t<li>112</li>\n",
       "\t<li>167</li>\n",
       "\t<li>2</li>\n",
       "\t<li>336</li>\n",
       "\t<li>385</li>\n",
       "\t<li>39</li>\n",
       "\t<li>4</li>\n",
       "\t<li>172</li>\n",
       "\t<li>4536</li>\n",
       "\t<li>1111</li>\n",
       "\t<li>17</li>\n",
       "\t<li>546</li>\n",
       "\t<li>38</li>\n",
       "\t<li>13</li>\n",
       "\t<li>447</li>\n",
       "\t<li>4</li>\n",
       "\t<li>192</li>\n",
       "\t<li>50</li>\n",
       "\t<li>16</li>\n",
       "\t<li>6</li>\n",
       "\t<li>147</li>\n",
       "\t<li>2025</li>\n",
       "\t<li>19</li>\n",
       "\t<li>14</li>\n",
       "\t<li>22</li>\n",
       "\t<li>4</li>\n",
       "\t<li>1920</li>\n",
       "\t<li>4613</li>\n",
       "\t<li>469</li>\n",
       "\t<li>4</li>\n",
       "\t<li>22</li>\n",
       "\t<li>71</li>\n",
       "\t<li>87</li>\n",
       "\t<li>12</li>\n",
       "\t<li>16</li>\n",
       "\t<li>43</li>\n",
       "\t<li>530</li>\n",
       "\t<li>38</li>\n",
       "\t<li>76</li>\n",
       "\t<li>15</li>\n",
       "\t<li>13</li>\n",
       "\t<li>1247</li>\n",
       "\t<li>4</li>\n",
       "\t<li>22</li>\n",
       "\t<li>17</li>\n",
       "\t<li>515</li>\n",
       "\t<li>17</li>\n",
       "\t<li>12</li>\n",
       "\t<li>16</li>\n",
       "\t<li>626</li>\n",
       "\t<li>18</li>\n",
       "\t<li>2</li>\n",
       "\t<li>5</li>\n",
       "\t<li>62</li>\n",
       "\t<li>386</li>\n",
       "\t<li>12</li>\n",
       "\t<li>8</li>\n",
       "\t<li>316</li>\n",
       "\t<li>8</li>\n",
       "\t<li>106</li>\n",
       "\t<li>5</li>\n",
       "\t<li>4</li>\n",
       "\t<li>2223</li>\n",
       "\t<li>5244</li>\n",
       "\t<li>16</li>\n",
       "\t<li>480</li>\n",
       "\t<li>66</li>\n",
       "\t<li>3785</li>\n",
       "\t<li>33</li>\n",
       "\t<li>4</li>\n",
       "\t<li>130</li>\n",
       "\t<li>12</li>\n",
       "\t<li>16</li>\n",
       "\t<li>38</li>\n",
       "\t<li>619</li>\n",
       "\t<li>5</li>\n",
       "\t<li>25</li>\n",
       "\t<li>124</li>\n",
       "\t<li>51</li>\n",
       "\t<li>36</li>\n",
       "\t<li>135</li>\n",
       "\t<li>48</li>\n",
       "\t<li>25</li>\n",
       "\t<li>1415</li>\n",
       "\t<li>33</li>\n",
       "\t<li>6</li>\n",
       "\t<li>22</li>\n",
       "\t<li>12</li>\n",
       "\t<li>215</li>\n",
       "\t<li>28</li>\n",
       "\t<li>77</li>\n",
       "\t<li>52</li>\n",
       "\t<li>5</li>\n",
       "\t<li>14</li>\n",
       "\t<li>407</li>\n",
       "\t<li>16</li>\n",
       "\t<li>82</li>\n",
       "\t<li>2</li>\n",
       "\t<li>8</li>\n",
       "\t<li>4</li>\n",
       "\t<li>107</li>\n",
       "\t<li>117</li>\n",
       "\t<li>5952</li>\n",
       "\t<li>15</li>\n",
       "\t<li>256</li>\n",
       "\t<li>4</li>\n",
       "\t<li>2</li>\n",
       "\t<li>7</li>\n",
       "\t<li>3766</li>\n",
       "\t<li>5</li>\n",
       "\t<li>723</li>\n",
       "\t<li>36</li>\n",
       "\t<li>71</li>\n",
       "\t<li>43</li>\n",
       "\t<li>530</li>\n",
       "\t<li>476</li>\n",
       "\t<li>26</li>\n",
       "\t<li>400</li>\n",
       "\t<li>317</li>\n",
       "\t<li>46</li>\n",
       "\t<li>7</li>\n",
       "\t<li>4</li>\n",
       "\t<li>2</li>\n",
       "\t<li>1029</li>\n",
       "\t<li>13</li>\n",
       "\t<li>104</li>\n",
       "\t<li>88</li>\n",
       "\t<li>4</li>\n",
       "\t<li>381</li>\n",
       "\t<li>15</li>\n",
       "\t<li>297</li>\n",
       "\t<li>98</li>\n",
       "\t<li>32</li>\n",
       "\t<li>2071</li>\n",
       "\t<li>56</li>\n",
       "\t<li>26</li>\n",
       "\t<li>141</li>\n",
       "\t<li>6</li>\n",
       "\t<li>194</li>\n",
       "\t<li>7486</li>\n",
       "\t<li>18</li>\n",
       "\t<li>4</li>\n",
       "\t<li>226</li>\n",
       "\t<li>22</li>\n",
       "\t<li>21</li>\n",
       "\t<li>134</li>\n",
       "\t<li>476</li>\n",
       "\t<li>26</li>\n",
       "\t<li>480</li>\n",
       "\t<li>5</li>\n",
       "\t<li>144</li>\n",
       "\t<li>30</li>\n",
       "\t<li>5535</li>\n",
       "\t<li>18</li>\n",
       "\t<li>51</li>\n",
       "\t<li>36</li>\n",
       "\t<li>28</li>\n",
       "\t<li>224</li>\n",
       "\t<li>92</li>\n",
       "\t<li>25</li>\n",
       "\t<li>104</li>\n",
       "\t<li>4</li>\n",
       "\t<li>226</li>\n",
       "\t<li>65</li>\n",
       "\t<li>16</li>\n",
       "\t<li>38</li>\n",
       "\t<li>1334</li>\n",
       "\t<li>88</li>\n",
       "\t<li>12</li>\n",
       "\t<li>16</li>\n",
       "\t<li>283</li>\n",
       "\t<li>5</li>\n",
       "\t<li>16</li>\n",
       "\t<li>4472</li>\n",
       "\t<li>113</li>\n",
       "\t<li>103</li>\n",
       "\t<li>32</li>\n",
       "\t<li>15</li>\n",
       "\t<li>16</li>\n",
       "\t<li>5345</li>\n",
       "\t<li>19</li>\n",
       "\t<li>178</li>\n",
       "\t<li>32</li>\n",
       "</ol>\n"
      ],
      "text/latex": [
       "\\begin{enumerate*}\n",
       "\\item 1\n",
       "\\item 14\n",
       "\\item 22\n",
       "\\item 16\n",
       "\\item 43\n",
       "\\item 530\n",
       "\\item 973\n",
       "\\item 1622\n",
       "\\item 1385\n",
       "\\item 65\n",
       "\\item 458\n",
       "\\item 4468\n",
       "\\item 66\n",
       "\\item 3941\n",
       "\\item 4\n",
       "\\item 173\n",
       "\\item 36\n",
       "\\item 256\n",
       "\\item 5\n",
       "\\item 25\n",
       "\\item 100\n",
       "\\item 43\n",
       "\\item 838\n",
       "\\item 112\n",
       "\\item 50\n",
       "\\item 670\n",
       "\\item 2\n",
       "\\item 9\n",
       "\\item 35\n",
       "\\item 480\n",
       "\\item 284\n",
       "\\item 5\n",
       "\\item 150\n",
       "\\item 4\n",
       "\\item 172\n",
       "\\item 112\n",
       "\\item 167\n",
       "\\item 2\n",
       "\\item 336\n",
       "\\item 385\n",
       "\\item 39\n",
       "\\item 4\n",
       "\\item 172\n",
       "\\item 4536\n",
       "\\item 1111\n",
       "\\item 17\n",
       "\\item 546\n",
       "\\item 38\n",
       "\\item 13\n",
       "\\item 447\n",
       "\\item 4\n",
       "\\item 192\n",
       "\\item 50\n",
       "\\item 16\n",
       "\\item 6\n",
       "\\item 147\n",
       "\\item 2025\n",
       "\\item 19\n",
       "\\item 14\n",
       "\\item 22\n",
       "\\item 4\n",
       "\\item 1920\n",
       "\\item 4613\n",
       "\\item 469\n",
       "\\item 4\n",
       "\\item 22\n",
       "\\item 71\n",
       "\\item 87\n",
       "\\item 12\n",
       "\\item 16\n",
       "\\item 43\n",
       "\\item 530\n",
       "\\item 38\n",
       "\\item 76\n",
       "\\item 15\n",
       "\\item 13\n",
       "\\item 1247\n",
       "\\item 4\n",
       "\\item 22\n",
       "\\item 17\n",
       "\\item 515\n",
       "\\item 17\n",
       "\\item 12\n",
       "\\item 16\n",
       "\\item 626\n",
       "\\item 18\n",
       "\\item 2\n",
       "\\item 5\n",
       "\\item 62\n",
       "\\item 386\n",
       "\\item 12\n",
       "\\item 8\n",
       "\\item 316\n",
       "\\item 8\n",
       "\\item 106\n",
       "\\item 5\n",
       "\\item 4\n",
       "\\item 2223\n",
       "\\item 5244\n",
       "\\item 16\n",
       "\\item 480\n",
       "\\item 66\n",
       "\\item 3785\n",
       "\\item 33\n",
       "\\item 4\n",
       "\\item 130\n",
       "\\item 12\n",
       "\\item 16\n",
       "\\item 38\n",
       "\\item 619\n",
       "\\item 5\n",
       "\\item 25\n",
       "\\item 124\n",
       "\\item 51\n",
       "\\item 36\n",
       "\\item 135\n",
       "\\item 48\n",
       "\\item 25\n",
       "\\item 1415\n",
       "\\item 33\n",
       "\\item 6\n",
       "\\item 22\n",
       "\\item 12\n",
       "\\item 215\n",
       "\\item 28\n",
       "\\item 77\n",
       "\\item 52\n",
       "\\item 5\n",
       "\\item 14\n",
       "\\item 407\n",
       "\\item 16\n",
       "\\item 82\n",
       "\\item 2\n",
       "\\item 8\n",
       "\\item 4\n",
       "\\item 107\n",
       "\\item 117\n",
       "\\item 5952\n",
       "\\item 15\n",
       "\\item 256\n",
       "\\item 4\n",
       "\\item 2\n",
       "\\item 7\n",
       "\\item 3766\n",
       "\\item 5\n",
       "\\item 723\n",
       "\\item 36\n",
       "\\item 71\n",
       "\\item 43\n",
       "\\item 530\n",
       "\\item 476\n",
       "\\item 26\n",
       "\\item 400\n",
       "\\item 317\n",
       "\\item 46\n",
       "\\item 7\n",
       "\\item 4\n",
       "\\item 2\n",
       "\\item 1029\n",
       "\\item 13\n",
       "\\item 104\n",
       "\\item 88\n",
       "\\item 4\n",
       "\\item 381\n",
       "\\item 15\n",
       "\\item 297\n",
       "\\item 98\n",
       "\\item 32\n",
       "\\item 2071\n",
       "\\item 56\n",
       "\\item 26\n",
       "\\item 141\n",
       "\\item 6\n",
       "\\item 194\n",
       "\\item 7486\n",
       "\\item 18\n",
       "\\item 4\n",
       "\\item 226\n",
       "\\item 22\n",
       "\\item 21\n",
       "\\item 134\n",
       "\\item 476\n",
       "\\item 26\n",
       "\\item 480\n",
       "\\item 5\n",
       "\\item 144\n",
       "\\item 30\n",
       "\\item 5535\n",
       "\\item 18\n",
       "\\item 51\n",
       "\\item 36\n",
       "\\item 28\n",
       "\\item 224\n",
       "\\item 92\n",
       "\\item 25\n",
       "\\item 104\n",
       "\\item 4\n",
       "\\item 226\n",
       "\\item 65\n",
       "\\item 16\n",
       "\\item 38\n",
       "\\item 1334\n",
       "\\item 88\n",
       "\\item 12\n",
       "\\item 16\n",
       "\\item 283\n",
       "\\item 5\n",
       "\\item 16\n",
       "\\item 4472\n",
       "\\item 113\n",
       "\\item 103\n",
       "\\item 32\n",
       "\\item 15\n",
       "\\item 16\n",
       "\\item 5345\n",
       "\\item 19\n",
       "\\item 178\n",
       "\\item 32\n",
       "\\end{enumerate*}\n"
      ],
      "text/markdown": [
       "1. 1\n",
       "2. 14\n",
       "3. 22\n",
       "4. 16\n",
       "5. 43\n",
       "6. 530\n",
       "7. 973\n",
       "8. 1622\n",
       "9. 1385\n",
       "10. 65\n",
       "11. 458\n",
       "12. 4468\n",
       "13. 66\n",
       "14. 3941\n",
       "15. 4\n",
       "16. 173\n",
       "17. 36\n",
       "18. 256\n",
       "19. 5\n",
       "20. 25\n",
       "21. 100\n",
       "22. 43\n",
       "23. 838\n",
       "24. 112\n",
       "25. 50\n",
       "26. 670\n",
       "27. 2\n",
       "28. 9\n",
       "29. 35\n",
       "30. 480\n",
       "31. 284\n",
       "32. 5\n",
       "33. 150\n",
       "34. 4\n",
       "35. 172\n",
       "36. 112\n",
       "37. 167\n",
       "38. 2\n",
       "39. 336\n",
       "40. 385\n",
       "41. 39\n",
       "42. 4\n",
       "43. 172\n",
       "44. 4536\n",
       "45. 1111\n",
       "46. 17\n",
       "47. 546\n",
       "48. 38\n",
       "49. 13\n",
       "50. 447\n",
       "51. 4\n",
       "52. 192\n",
       "53. 50\n",
       "54. 16\n",
       "55. 6\n",
       "56. 147\n",
       "57. 2025\n",
       "58. 19\n",
       "59. 14\n",
       "60. 22\n",
       "61. 4\n",
       "62. 1920\n",
       "63. 4613\n",
       "64. 469\n",
       "65. 4\n",
       "66. 22\n",
       "67. 71\n",
       "68. 87\n",
       "69. 12\n",
       "70. 16\n",
       "71. 43\n",
       "72. 530\n",
       "73. 38\n",
       "74. 76\n",
       "75. 15\n",
       "76. 13\n",
       "77. 1247\n",
       "78. 4\n",
       "79. 22\n",
       "80. 17\n",
       "81. 515\n",
       "82. 17\n",
       "83. 12\n",
       "84. 16\n",
       "85. 626\n",
       "86. 18\n",
       "87. 2\n",
       "88. 5\n",
       "89. 62\n",
       "90. 386\n",
       "91. 12\n",
       "92. 8\n",
       "93. 316\n",
       "94. 8\n",
       "95. 106\n",
       "96. 5\n",
       "97. 4\n",
       "98. 2223\n",
       "99. 5244\n",
       "100. 16\n",
       "101. 480\n",
       "102. 66\n",
       "103. 3785\n",
       "104. 33\n",
       "105. 4\n",
       "106. 130\n",
       "107. 12\n",
       "108. 16\n",
       "109. 38\n",
       "110. 619\n",
       "111. 5\n",
       "112. 25\n",
       "113. 124\n",
       "114. 51\n",
       "115. 36\n",
       "116. 135\n",
       "117. 48\n",
       "118. 25\n",
       "119. 1415\n",
       "120. 33\n",
       "121. 6\n",
       "122. 22\n",
       "123. 12\n",
       "124. 215\n",
       "125. 28\n",
       "126. 77\n",
       "127. 52\n",
       "128. 5\n",
       "129. 14\n",
       "130. 407\n",
       "131. 16\n",
       "132. 82\n",
       "133. 2\n",
       "134. 8\n",
       "135. 4\n",
       "136. 107\n",
       "137. 117\n",
       "138. 5952\n",
       "139. 15\n",
       "140. 256\n",
       "141. 4\n",
       "142. 2\n",
       "143. 7\n",
       "144. 3766\n",
       "145. 5\n",
       "146. 723\n",
       "147. 36\n",
       "148. 71\n",
       "149. 43\n",
       "150. 530\n",
       "151. 476\n",
       "152. 26\n",
       "153. 400\n",
       "154. 317\n",
       "155. 46\n",
       "156. 7\n",
       "157. 4\n",
       "158. 2\n",
       "159. 1029\n",
       "160. 13\n",
       "161. 104\n",
       "162. 88\n",
       "163. 4\n",
       "164. 381\n",
       "165. 15\n",
       "166. 297\n",
       "167. 98\n",
       "168. 32\n",
       "169. 2071\n",
       "170. 56\n",
       "171. 26\n",
       "172. 141\n",
       "173. 6\n",
       "174. 194\n",
       "175. 7486\n",
       "176. 18\n",
       "177. 4\n",
       "178. 226\n",
       "179. 22\n",
       "180. 21\n",
       "181. 134\n",
       "182. 476\n",
       "183. 26\n",
       "184. 480\n",
       "185. 5\n",
       "186. 144\n",
       "187. 30\n",
       "188. 5535\n",
       "189. 18\n",
       "190. 51\n",
       "191. 36\n",
       "192. 28\n",
       "193. 224\n",
       "194. 92\n",
       "195. 25\n",
       "196. 104\n",
       "197. 4\n",
       "198. 226\n",
       "199. 65\n",
       "200. 16\n",
       "201. 38\n",
       "202. 1334\n",
       "203. 88\n",
       "204. 12\n",
       "205. 16\n",
       "206. 283\n",
       "207. 5\n",
       "208. 16\n",
       "209. 4472\n",
       "210. 113\n",
       "211. 103\n",
       "212. 32\n",
       "213. 15\n",
       "214. 16\n",
       "215. 5345\n",
       "216. 19\n",
       "217. 178\n",
       "218. 32\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "  [1]    1   14   22   16   43  530  973 1622 1385   65  458 4468   66 3941    4\n",
       " [16]  173   36  256    5   25  100   43  838  112   50  670    2    9   35  480\n",
       " [31]  284    5  150    4  172  112  167    2  336  385   39    4  172 4536 1111\n",
       " [46]   17  546   38   13  447    4  192   50   16    6  147 2025   19   14   22\n",
       " [61]    4 1920 4613  469    4   22   71   87   12   16   43  530   38   76   15\n",
       " [76]   13 1247    4   22   17  515   17   12   16  626   18    2    5   62  386\n",
       " [91]   12    8  316    8  106    5    4 2223 5244   16  480   66 3785   33    4\n",
       "[106]  130   12   16   38  619    5   25  124   51   36  135   48   25 1415   33\n",
       "[121]    6   22   12  215   28   77   52    5   14  407   16   82    2    8    4\n",
       "[136]  107  117 5952   15  256    4    2    7 3766    5  723   36   71   43  530\n",
       "[151]  476   26  400  317   46    7    4    2 1029   13  104   88    4  381   15\n",
       "[166]  297   98   32 2071   56   26  141    6  194 7486   18    4  226   22   21\n",
       "[181]  134  476   26  480    5  144   30 5535   18   51   36   28  224   92   25\n",
       "[196]  104    4  226   65   16   38 1334   88   12   16  283    5   16 4472  113\n",
       "[211]  103   32   15   16 5345   19  178   32"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_data[[1]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Las reseñas de peliculas pueden tener diferentes longitudes. El siguiente codigo muestra el numero de palabras en la primera y segunda reseña. Dado que las entradas a una red neuronal deben tener la misma lingitud, se tiene que resolver esto mas adelante."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<ol class=list-inline>\n",
       "\t<li>218</li>\n",
       "\t<li>189</li>\n",
       "</ol>\n"
      ],
      "text/latex": [
       "\\begin{enumerate*}\n",
       "\\item 218\n",
       "\\item 189\n",
       "\\end{enumerate*}\n"
      ],
      "text/markdown": [
       "1. 218\n",
       "2. 189\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "[1] 218 189"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sapply(train_data[1:2],length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convertir enteros a palabras\n",
    "\n",
    "Puede se util saber como convertir enteros de nuevo a texto. Ya se sabe que word_index tiene el catalogo de palabra. Si se crea un data.frame a partir de el, se puede usar en ambas direcciones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_index_df<-data.frame(\n",
    "  word = names(word_index),\n",
    "  idx = unlist(word_index,use.names = FALSE),\n",
    "  stringsAsFactors = FALSE\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los primeros índices son reservados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_index_df <- word_index_df%>%\n",
    "  mutate(idx = idx + 3)%>%\n",
    "  add_row(word = \"<PAD>\", idx = 0)%>%\n",
    "  add_row(word = \"<START>\", idx = 1)%>%\n",
    "  add_row(word = \"<UNK>\", idx = 2)%>%\n",
    "  add_row(word = \"<UNUSED>\", idx = 3)%>%\n",
    "  arrange(idx)\n",
    "\n",
    "decode_review <- function(text){\n",
    "  paste(map(text, function(number) word_index_df %>%\n",
    "              filter(idx == number) %>%\n",
    "              select(word) %>% \n",
    "              pull()),\n",
    "        collapse = \" \")\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora se puede usar la funcion decode_view para mostrar el texto para la primera reseña"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "'&lt;START&gt; this film was just brilliant casting location scenery story direction everyone\\'s really suited the part they played and you could just imagine being there robert &lt;UNK&gt; is an amazing actor and now the same being director &lt;UNK&gt; father came from the same scottish island as myself so i loved the fact there was a real connection with this film the witty remarks throughout the film were great it was just brilliant so much that i bought the film as soon as it was released for &lt;UNK&gt; and would recommend it to everyone to watch and the fly fishing was amazing really cried at the end it was so sad and you know what they say if you cry at a film it must have been good and this definitely was also &lt;UNK&gt; to the two little boy\\'s that played the &lt;UNK&gt; of norman and paul they were just brilliant children are often left out of the &lt;UNK&gt; list i think because the stars that play them all grown up are such a big profile for the whole film but these children are amazing and should be praised for what they have done don\\'t you think the whole story was so lovely because it was true and was someone\\'s life after all that was shared with us all'"
      ],
      "text/latex": [
       "'<START> this film was just brilliant casting location scenery story direction everyone\\textbackslash{}'s really suited the part they played and you could just imagine being there robert <UNK> is an amazing actor and now the same being director <UNK> father came from the same scottish island as myself so i loved the fact there was a real connection with this film the witty remarks throughout the film were great it was just brilliant so much that i bought the film as soon as it was released for <UNK> and would recommend it to everyone to watch and the fly fishing was amazing really cried at the end it was so sad and you know what they say if you cry at a film it must have been good and this definitely was also <UNK> to the two little boy\\textbackslash{}'s that played the <UNK> of norman and paul they were just brilliant children are often left out of the <UNK> list i think because the stars that play them all grown up are such a big profile for the whole film but these children are amazing and should be praised for what they have done don\\textbackslash{}'t you think the whole story was so lovely because it was true and was someone\\textbackslash{}'s life after all that was shared with us all'"
      ],
      "text/markdown": [
       "'&lt;START&gt; this film was just brilliant casting location scenery story direction everyone\\'s really suited the part they played and you could just imagine being there robert &lt;UNK&gt; is an amazing actor and now the same being director &lt;UNK&gt; father came from the same scottish island as myself so i loved the fact there was a real connection with this film the witty remarks throughout the film were great it was just brilliant so much that i bought the film as soon as it was released for &lt;UNK&gt; and would recommend it to everyone to watch and the fly fishing was amazing really cried at the end it was so sad and you know what they say if you cry at a film it must have been good and this definitely was also &lt;UNK&gt; to the two little boy\\'s that played the &lt;UNK&gt; of norman and paul they were just brilliant children are often left out of the &lt;UNK&gt; list i think because the stars that play them all grown up are such a big profile for the whole film but these children are amazing and should be praised for what they have done don\\'t you think the whole story was so lovely because it was true and was someone\\'s life after all that was shared with us all'"
      ],
      "text/plain": [
       "[1] \"<START> this film was just brilliant casting location scenery story direction everyone's really suited the part they played and you could just imagine being there robert <UNK> is an amazing actor and now the same being director <UNK> father came from the same scottish island as myself so i loved the fact there was a real connection with this film the witty remarks throughout the film were great it was just brilliant so much that i bought the film as soon as it was released for <UNK> and would recommend it to everyone to watch and the fly fishing was amazing really cried at the end it was so sad and you know what they say if you cry at a film it must have been good and this definitely was also <UNK> to the two little boy's that played the <UNK> of norman and paul they were just brilliant children are often left out of the <UNK> list i think because the stars that play them all grown up are such a big profile for the whole film but these children are amazing and should be praised for what they have done don't you think the whole story was so lovely because it was true and was someone's life after all that was shared with us all\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "decode_review(train_data[[1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepacion de datos\n",
    "\n",
    "Las reseña, los arreglos de enteros, deben transformarse a tensores antes de alimentar la red neuronal. Esta convension se puede realizar de dos maneras. Codificando en caliente las matrices para transformarlas en vector de 0's y 1's. Por ejemplo, se secuencia [3,5] se tranformaria en un vector de dimension 10,000 que son todos ceros, excepto los indices 3 y 5, que son unos. \n",
    "\n",
    "Entonces, se hace esta la primera capa de la red, una capa densa, que pueda manejar datos de vectores de punto flotante. Sin embargo, este enfoque requiere mucha memoria y requiere un matriz de tamaño num_words * num_reviews(reseñas). De manera alternativa, se puede rellenar los areglos de enteros de maenra que tengan la misma longitud, y luego crear un tensor de tamaño num_examples * max_length. Se puede usar una cada de inclusion capaz de manejar esta forma como la primera capa en la red.\n",
    "\n",
    "Dedibo a que las reseñas deben dener la misma longitud, se utilizara la funcion pad_sequences para estanzarizar las longitudes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data<-pad_sequences(\n",
    "  train_data,\n",
    "  value = word_index_df%>%filter(word == \"<PAD>\")%>%select(idx)%>%pull(),\n",
    "  padding =  \"post\",\n",
    "  maxlen = 256\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construccion del modelo\n",
    "\n",
    "La red neuronal se crea apilando capas, esto requiere dos decisiones arquitectonicas principales: ¿cuántas capas usar en el modelo? ¿cuántas unidades ocultas usar para cada capa?\n",
    " \n",
    "En este emjemplo, los datos de entrada consisten en una matriz de indices de palabras. Las etiquetas para predecir si son 0 o 1. \n",
    "\n",
    "Construyamos un modelo para este problema.\n",
    "La forma de entrada es un contenceo de palabras usado para la reseña de la pelicual (10,000 palabras)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "________________________________________________________________________________\n",
      "Layer (type)                        Output Shape                    Param #     \n",
      "================================================================================\n",
      "embedding (Embedding)               (None, None, 16)                160000      \n",
      "________________________________________________________________________________\n",
      "global_average_pooling1d (GlobalAve (None, 16)                      0           \n",
      "________________________________________________________________________________\n",
      "dense (Dense)                       (None, 16)                      272         \n",
      "________________________________________________________________________________\n",
      "dense_1 (Dense)                     (None, 1)                       17          \n",
      "================================================================================\n",
      "Total params: 160,289\n",
      "Trainable params: 160,289\n",
      "Non-trainable params: 0\n",
      "________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "vocab_size<-10000\n",
    "\n",
    "model<-keras_model_sequential()\n",
    "\n",
    "model%>%\n",
    "  layer_embedding(input_dim = vocab_size, output_dim = 16)%>%\n",
    "  layer_global_average_pooling_1d()%>%\n",
    "  layer_dense(units = 16, activation = \"relu\")%>%\n",
    "  layer_dense(units = 1, activation = \"sigmoid\")\n",
    "\n",
    "model%>%summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Las capas se apilan de manera secuencial para construir el clasificador:\n",
    "\n",
    "La primer capa es una cama embebida. Esta capa toma el vocabulario codificado con enteros y busca el vector de incrustacion para ca indice de palabras. Estos vectores se aprender a medida que el modelo entrena.\n",
    "Los vectores agregan una dimension a la matriz de salida. Las dimensiones resultanes son: (tach, sequence, embedding).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "model %>% compile(\n",
    "  optimizer = 'adam',\n",
    "  loss = 'binary_crossentropy',\n",
    "  metrics = list('accuracy')\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Crear un conjunto de validación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_val <- train_data[1:10000, ]\n",
    "partial_x_train <- train_data[10001:nrow(train_data), ]\n",
    "\n",
    "y_val <- train_labels[1:10000]\n",
    "partial_y_train <- train_labels[10001:length(train_labels)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entrenamos el modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "history <- model %>% fit(\n",
    "  partial_x_train,\n",
    "  partial_y_train,\n",
    "  epochs = 40,\n",
    "  batch_size = 512,\n",
    "  validation_data = list(x_val, y_val),\n",
    "  verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluar el modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results <- model %>% evaluate(test_data, test_labels)\n",
    "results\n",
    "\n",
    "\n",
    "predictions<-model%>%predict(test_data)\n",
    "\n",
    "cbind(round(predictions,5),test_labels)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
